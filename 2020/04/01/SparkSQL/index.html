<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=0"><meta name="description" content="SparkSQL [ Friend ] "><meta name="theme-color" content="#ebc65a"><title>SparkSQL [ Friend ] </title><link rel="shortcut icon" href="/favicon.ico"><link rel="stylesheet" href="/css/index.css"><script src="https://cdn.bootcss.com/highlight.js/9.6.0/highlight.min.js" defer></script><script src="/js/paper.js" defer></script><script src="https://www.googletagmanager.com/gtag/js?id=xx-xxxxxxx-xx" async></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'xx-xxxxxxx-xx');</script><link rel="stylesheet" href="/css/tocbot.css"><script src="/js/tocbot.js" defer></script><script>window.addEventListener('DOMContentLoaded', () => {
  tocbot.init({
    // Where to render the table of contents.
    tocSelector: '.toc__content',
    // Where to grab the headings to build the table of contents.
    contentSelector: '.article__content',
    // Which headings to grab inside of the contentSelector element.
    headingSelector: 'h1, h2, h3, h4, h5, h6',
    // For headings inside relative or absolute positioned containers within content.
    hasInnerContainers: true,
    orderedList: false,
    collapseDepth: 2,
  });
})</script><link rel="preload" href="https://cdn.bootcss.com/highlight.js/9.6.0/styles/github.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><link rel="stylesheet" href="https://cdn.bootcss.com/highlight.js/9.6.0/styles/github.min.css"><link rel="preload" href="https://fonts.googleapis.com/css?family=Abril+Fatface&amp;display=swap" as="style" onload="this.onload=null;this.rel='stylesheet'"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Abril+Fatface&amp;display=swap"></head><body><div class="mask-border"></div><div class="head"><div class="head__inner"><h1><a href="/">Friend</a></h1><p></p></div></div><div class="paper-container"><div class="main"><div class="location-bar"><div class="line-1"><div class="horizontal-line" style="height: 3px"></div></div><div class="line-2"><div class="horizontal-line" style="height: 1px"></div></div><p class="text">SparkSQL</p><div class="switch-button"><input class="container_toggle" type="checkbox" id="switch" name="mode"><label for="switch">Toggle</label></div><div class="line-3"><div class="horizontal-line" style="height: 1px"></div></div></div><div class="main__2-col"><article class="post-view__article"><div class="article__infomation"><div class="posts-item"><h2 class="posts-item__title"><a href="">SparkSQL</a></h2><span class="post__date">2020-04-01</span></div></div><div class="article__content"><h1 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a>Spark SQL</h1><h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><center class="half"><br><img src="/2020/04/01/SparkSQL/架构图.jpg"><br></center>

<p><strong>语言API</strong></p>
<p>Spark与不同的语言和Spark SQL兼容。 它也是由这些语言支持的API（python，scala，java，HiveQL）。</p>
<p><strong>模式RDD</strong></p>
<p>Spark Core是使用称为RDD的特殊数据结构设计的。 通常，Spark SQL适用于模式，表和记录。 因此，我们可以使用Schema RDD作为临时表。 我们可以将此Schema RDD称为数据帧。</p>
<p><strong>数据源</strong></p>
<p>通常spark-core的数据源是文本文件，Avro文件等。但是，Spark SQL的数据源不同。 这些是Parquet文件，JSON文档，HIVE表和Cassandra数据库。</p>
<h1 id="Spark-SQL的特点"><a href="#Spark-SQL的特点" class="headerlink" title="Spark SQL的特点"></a>Spark SQL的特点</h1><ol>
<li>容易集成</li>
<li>统一的数据访问方式</li>
<li>兼容Hive</li>
</ol>
<h1 id="DataFrame和DataSet"><a href="#DataFrame和DataSet" class="headerlink" title="DataFrame和DataSet"></a>DataFrame和DataSet</h1><ol>
<li><p>DataFrame</p>
<p>DataFrame是组织成命名列的<strong>数据集</strong>。它在概念上等同于关系数据库中的<strong>表</strong>，但在底层具有更丰富的优化。DataFrames可以从各种来源构建，</p>
<p>例如：</p>
<p>　　结构化数据文件</p>
<p>　　hive中的表</p>
<p>　　外部数据库或现有RDDs</p>
<p>DataFrame API支持的语言有Scala，Java，Python和R。</p>
<center class="half"><br><img src="/2020/04/01/SparkSQL/RDD和DataFrame.png"><br></center>

<p>从上图可以看出，DataFrame多了数据的结构信息，<strong>即schema</strong>。RDD是分布式的 Java对象的集合。DataFrame是分布式的Row对象的集合。DataFrame除了提供了比RDD更丰富的算子以外，更重要的特点是提升执行效率、减少数据读取以及执行计划的优化。</p>
</li>
<li><p>DataSet</p>
<p>Dataset是数据的分布式集合。Dataset是在Spark 1.6中添加的一个新接口，是DataFrame之上更高一级的抽象。它提供了RDD的优点（强类型化，使用强大的lambda函数的能力）以及Spark SQL优化后的执行引擎的优点。一个Dataset 可以从JVM对象构造，然后使用函数转换（map， flatMap，filter等）去操作。 Dataset API 支持Scala和Java。 Python不支持Dataset API。</p>
</li>
</ol>
<h1 id="主要介绍一下DataSet"><a href="#主要介绍一下DataSet" class="headerlink" title="主要介绍一下DataSet"></a>主要介绍一下DataSet</h1><p>Dataset是一个分布式的数据收集器。这是在Spark1.6之后新加的一个接口，兼顾了RDD的优点（<strong>强类型</strong>，可以使用功能强大的lambda）以及Spark SQL的执行器高效性的优点。所以可以把DataFrames看成是一种特殊的Datasets，即：Dataset(Row)</p>
<center class="half"><br><img src="/2020/04/01/SparkSQL/DataSet.png"><br></center>

<p><strong>创建DataSet，方式一：使用序列</strong></p>
<ol>
<li><p>定义case class</p>
<p><code>case class MyData(a:Int,b:String)</code></p>
</li>
<li><p>生成序列，并创建DataSet</p>
<p><code>val ds = Seq(MyData(1,&quot;Tom&quot;),MyData(2,&quot;Mary&quot;)).toDS</code></p>
</li>
<li><p>查看结果<br><img src="https://img2018.cnblogs.com/blog/1367933/201810/1367933-20181025100740657-1854671955.png" alt="img"></p>
</li>
</ol>
<p><strong>创建DataSet，方式二：使用JSON数据</strong></p>
<ol>
<li>定义case class</li>
</ol>
<p>​       <code>case class Person(name: String, gender: String)</code></p>
<ol start="2">
<li>通过JSON数据生成DataFrame</li>
</ol>
<p>​       <code>val df = spark.read.json(sc.parallelize(&quot;&quot;&quot;{&quot;gender&quot;: &quot;Male&quot;, &quot;name&quot;: &quot;Tom&quot;}&quot;&quot;&quot; :: Nil))</code></p>
<ol start="3">
<li>将DataFrame转成DataSet</li>
</ol>
<p>​       <code>df.as[Person].show</code></p>
<p>​        <code>df.as[Person].collect</code></p>
<p><strong>创建DataSet，方式三：使用HDFS数据</strong></p>
<ol>
<li>读取HDFS数据，并创建DataSet</li>
</ol>
<p>​        <code>val linesDS = spark.read.text(&quot;hdfs://hadoop111:9000/data/data.txt&quot;).as[String]</code> </p>
<ol start="2">
<li>对DataSet进行操作：分词后，查询长度大于3的单词</li>
</ol>
<p>​        <code>val words = linesDS.flatMap(_.split(&quot; &quot;)).filter(_.length &gt; 3)</code></p>
<p>​        <code>words.show</code></p>
<p>​        <code>words.collect</code></p>
<ol start="3">
<li>执行WordCount程序</li>
</ol>
<p>​       <code>val result = linesDS.flatMap(_.split(&quot; &quot;)).map((_,1)).groupByKey(x =&gt; x._1).count</code></p>
<p>​       <code>result.show</code></p>
<p>​       排序：<code>result.orderBy($&quot;value&quot;).show</code></p>
</div></article><div class="post-view__sidebar"><div class="sidebar"><div class="tocbot"><h2>Toc</h2><div class="toc__content"></div></div><h2>Archives</h2><div class="sidebar__archives"><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/">2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/">2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/">2018</a></li></ul></div><h2>Categories</h2><div class="sidebar__categories"></div><h2>Tags</h2><div class="sidebar__tags"><ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Mac/">Mac</a></li></ul></div></div></div></div><div class="horizontal-line" style="height: 1px"></div><div class="main__bottom"><div class="pre-next"><a class="pre-button" href="/2020/04/02/Spark中的UDAF/">Spark中的UDAF</a><a class="next-button" href="/2020/03/31/Spark中的RDD/">Spark中的RDD</a></div></div></div></div><div class="footer"><span>©️2019-2020 Designed By&nbsp;<strong><a href="https://github.com/random-yang">RandomYang</a></strong> Powered By&nbsp;</span><strong><a href="https://hexo.io">hexo</a></strong></div><div class="darkmode-mask" id="darkmode-mask"></div><div class="sidebar__button"></div></body></html>